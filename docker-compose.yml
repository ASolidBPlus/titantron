services:
  titantron:
    build: .
    ports:
      - "8765:8765"
    volumes:
      - titantron-config:/config
      # Mount video libraries for visual analysis (scene detection):
      # - /path/to/media:/media:ro
      # Then in Admin > Settings > Path Mapping, set:
      #   Jellyfin Path Prefix: /data/videos  (Jellyfin's internal path)
      #   Backend Mount:        /media         (this container's mount point)
    env_file:
      - .env
    environment:
      - TITANTRON_DB_PATH=/config/titantron.db
      - TITANTRON_FRONTEND_DIR=/app/frontend-build
    restart: unless-stopped

  # ── ML-powered music detection (optional) ──
  # Uncomment to enable, then set ML Service URL in Admin > Settings
  # to http://titantron-ml:8769
  # First start installs dependencies (~2 min), cached in volume for subsequent starts.
  # Auto-detects CUDA — add deploy.resources for GPU support.
  # titantron-ml:
  #   image: ghcr.io/asolidbplus/titantron-ml:latest
  #   volumes:
  #     - ml-cache:/cache  # persists installed deps + downloaded model
  #     # Mount video files for audio extraction (same source as titantron):
  #     # - /path/to/media:/media:ro
  #     # Then in Admin > Settings > Path Mapping, set:
  #     #   ML Container Mount: /media  (this container's mount point)
  #   restart: unless-stopped
  #   # Uncomment for GPU support (requires nvidia-container-toolkit):
  #   # deploy:
  #   #   resources:
  #   #     reservations:
  #   #       devices:
  #   #         - capabilities: [gpu]

volumes:
  titantron-config:
  # ml-cache:  # uncomment with titantron-ml
